## [Implement ReLU Activation Function](https://www.deep-ml.com/problems/42) ![Difficulty](https://img.shields.io/badge/-Easy-brightgreen)

Write a Python function relu that implements the Rectified Linear Unit (ReLU) activation function. The function should take a single float as input and return the value after applying the ReLU function. The ReLU function returns the input if it's greater than 0, otherwise, it returns 0.

### Example:

**Input:**

```
print(relu(0)) 
print(relu(1)) 
print(relu(-1))
```


**Output:**

```
0
1
0
```
