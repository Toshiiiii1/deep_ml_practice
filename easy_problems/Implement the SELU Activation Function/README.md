## [Implement the SELU Activation Function](https://www.deep-ml.com/problems/103) ![Difficulty](https://img.shields.io/badge/-Easy-brightgreen)

Implement the SELU (Scaled Exponential Linear Unit) activation function, a self-normalizing variant of ELU. Your task is to compute the SELU value for a given input while ensuring numerical stability.

### Example:

**Input:**

```python
selu(-1.0)
```


**Output:**

```-1.1113```
