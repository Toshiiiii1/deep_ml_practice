## [Implement the PReLU Activation Function](https://www.deep-ml.com/problems/98) ![Difficulty](https://img.shields.io/badge/-Easy-brightgreen)

Implement the PReLU (Parametric ReLU) activation function, a variant of the ReLU activation function that introduces a learnable parameter for negative inputs. Your task is to compute the PReLU activation value for a given input.

### Example:

**Input:**

```python
prelu(-2.0, alpha=0.25)
```


**Output:**

```-0.5```
