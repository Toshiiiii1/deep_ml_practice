## [Implement the Softplus Activation Function](https://www.deep-ml.com/problems/99) ![Difficulty](https://img.shields.io/badge/-Easy-brightgreen)

Implement the Softplus activation function, a smooth approximation of the ReLU function. Your task is to compute the Softplus value for a given input, handling edge cases to prevent numerical overflow or underflow.

### Example:

**Input:**

```python
softplus(2)
```


**Output:**

```2.1269```
