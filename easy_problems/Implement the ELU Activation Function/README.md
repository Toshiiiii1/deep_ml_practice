## [Implement the ELU Activation Function](https://www.deep-ml.com/problems/97) ![Difficulty](https://img.shields.io/badge/-Easy-brightgreen)

Implement the ELU (Exponential Linear Unit) activation function, which helps mitigate the limitations of ReLU by providing negative outputs for negative inputs. The function should compute the ELU activation value for a given input.

### Example:

**Input:**

```python
elu(-1)
```


**Output:**

```-0.6321```
